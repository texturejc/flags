{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe5372f-5482-4a2d-9c95-3cb76904b6e7",
   "metadata": {},
   "source": [
    "# Computationally measuring image texture \n",
    "\n",
    "![Texture](texture_patches.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mahotas\n",
    "!pip install pycountry\n",
    "!pip install countryinfo\n",
    "!pip install pingouin\n",
    "#pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import mahotas as mt\n",
    "import glob\n",
    "import os\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "import numpy as np\n",
    "import pycountry\n",
    "from countryinfo import CountryInfo\n",
    "import pingouin as pg\n",
    "from IPython.display import Image, display\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pingouin as pg\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87e2ab",
   "metadata": {},
   "source": [
    "# What are Haralick image texture features?\n",
    "\n",
    "An image texture is a small-scale image feature that is determined by the spatial arrangement of pixel intensities. A texture does <b>not</b> depend on what an image depicts, but the two are often associated (images that depict large numbers of small particles will often have a similar texture, for instance).\n",
    "\n",
    "Haralick features are a set of image texture descriptors derived from the Gray Level Co-occurrence Matrix (GLCM), which is a method of examining the texture of an image by considering the spatial relationship of pixels. The GLCM is a matrix that represents how often different combinations of pixel brightness values (gray levels) occur in an image. From the GLCM, various statistical measures (Haralick features) can be calculated to describe the texture of the image. There are 14 commonly used Haralick features, each capturing different aspects of texture:\n",
    "\n",
    "1. Angular Second Moment (ASM): Measures image homogeneity. Higher values indicate more homogeneity or uniformity in the image texture.\n",
    "\n",
    "2. Contrast: Measures the local variations in the gray-level co-occurrence matrix. Higher contrast values indicate greater disparities in pixel intensities.\n",
    "\n",
    "3. Correlation: Evaluates the joint probability occurrence of the specified pixel pairs. High correlation indicates a predictable relationship between pixel values.\n",
    "\n",
    "4. Sum of Squares: Variance: Reflects the variance of the image intensities. It's a measure of the spread or dispersion of pixel values.\n",
    "\n",
    "5. Inverse Difference Moment (IDM): Also known as Homogeneity. It's high when the image has less contrast, indicating more homogeneity.\n",
    "\n",
    "6. Sum Average: The average value of the sum of gray levels of pixel pairs. It's a measure of the overall brightness.\n",
    "\n",
    "7. Sum Variance: Measures the variance of the sum of the GLCM. It assesses the variance in the sum average.\n",
    "\n",
    "8. Sum Entropy: Measures the randomness or complexity in the sum of gray levels. Higher values indicate more complexity.\n",
    "\n",
    "9. Entropy: Quantifies the disorder or complexity of the image. Higher entropy values imply more complex texture patterns.\n",
    "\n",
    "10. Difference Variance: Measures the variance in the difference between the gray levels of the pixel pairs.\n",
    "\n",
    "11. Difference Entropy: Measures the complexity or randomness of the differences between the gray levels of the pixel pairs.\n",
    "\n",
    "12. Information Measures of Correlation I & II: These two features provide information about the complexity of the image texture as seen in the GLCM. They measure how correlated a pixel is to its neighbor over the whole image.\n",
    "\n",
    "13. Maximal Correlation Coefficient (MCC): This measures the correlation between the probabilities of the pixel pairs. It requires eigenvalue calculations and is often more computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e322962-3f2e-45bf-9d85-811ada54cf69",
   "metadata": {},
   "source": [
    "## Image entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d33f7a-7183-453d-a7b2-80a5364bb3ba",
   "metadata": {},
   "source": [
    "### 1. Shannon entropy\n",
    "\n",
    "Let's start with entropy. You'll remember that entropy measures how predictable a probability distribution is. The formula for entropy is:\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i)\n",
    "$$\n",
    "\n",
    "Here, $H(X)$ gives the entropy in bits, where $x_i \\in X$ is an item in a discrete probability distribution. But how can we apply this to an image? The first step comes with recognising that images have what's known as a $bit$ encoding. This gives the levels of intensity a pixel of an image can take. The most common $bit$ encoding is 8-$bit$ encoding, which allows 256 ($2^8 = 256$) levels of intensity. This means that every image can be thought of as a histogram, where the bars of the histogram represent the counts of the pixels at each level of intensity. In this coding scheme, the $0^{th}$ intensity is black and the $255^{th}$ intensity is white.\n",
    "\n",
    "![Virginia Woolf](woolf.jpeg)\n",
    "\n",
    "Let's take an example image. This 8-$bit$ image consists of 50,400 pixels, and has $width \\times height$ dimensions of $180 \\times 280$ pixels. Extracting the intensty of each of these pixels gives the following counts:\n",
    "\n",
    "![Woolf histogram](vwoolf_histogram.jpeg)\n",
    "\n",
    "If we were to pick a pixel at random from this image, what intensity has the highest prolability of being picked? By thinking of the image in this way, we can represent it as a probability distribution across pixel intensities––and this allows us to use calculate the entropy of an image. If a specific intensity dominates, the image will have low entropy (it is very predictable); if a lot of intensities occur with equal frequency, it will have high entropy.\n",
    "\n",
    "<b>The Shannon entropy of this image is 7.65 $bits$.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a1eec-3985-40c7-94e5-72c716d4866c",
   "metadata": {},
   "source": [
    "### 2. Haralick entropy\n",
    "\n",
    "Though Shannon entropy is useful, it needs to be refined to capture image texture. This is because image texture is defined across the <b>differences</b> between pixels. The Haralick implementation of entropy captures this by way of what's known as the gray level cooccurrence matrix (GLCM). This looks at how often a transition between two pixel intensities occurs in four directions: horizontal, vertical, and two diagonals.\n",
    "\n",
    "![GLCM](GCLM.jpeg)\n",
    "\n",
    "The actual GLCM is defined as a $N \\times N$ matrix, where $N$ is the number of intensities and each entry is the number of times that the $(i,j)$ pair occurs. There are four GLCMs for every image, but these are often averged to give a single GLCM. \n",
    "\n",
    "The Haralick entropy formula works by getting the expected value of the surprise of every co-occurring pair of pixel intensities $(i,j)$ in all the GLCMs in the image. Here, $N$ is the number of possible pixel intensities, which in an 8-$bit$ image is 256. \n",
    "\n",
    "$$\n",
    "H = -\\sum_{i=0}^{N_g-1} \\sum_{j=0}^{N_g-1} P(i, j) \\log_2 P(i, j)\n",
    "$$\n",
    "\n",
    "Note that if there is no GLCM where $i$ and $j$ are adjacent, then $P(i,j)$ will be zero. (Edge and corner pixels that do not have eight neighbours are ignored.)\n",
    "\n",
    "![Virginia Woolf](woolf.jpeg)\n",
    "\n",
    "<b>The Haralick entropy of this image is 12.4 $bits$.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838d728-2fdc-4233-8e76-6de04defe653",
   "metadata": {},
   "source": [
    "## Image contrast\n",
    "\n",
    "Contrast measures the difference in intensity between the brightest and darkest parts of an image. It's formula is:\n",
    "\n",
    "$$\n",
    "\\text{Contrast} = \\sum_{i=0}^{N_g-1} \\sum_{j=0}^{N_g-1} P(i, j) (i - j)^2\n",
    "$$\n",
    "\n",
    "Like Haralick entropy, all possible pairs of pixel intensities $(i,j)$ are assigned a probability $P(i,j)$, with this probability only being greater than zero if a pixel with intensity $i$ occurs next to a pixel with intensity $j$ in a GCLM. This is then multiplied by the square of the difference in intensity, so that big differences are 'rewarded' and small differences are 'punished'. In other words, we take the expected value of the differences in intensity between adjacent pixels. \n",
    "\n",
    "![Eliot](eliot.jpg)\n",
    "\n",
    "<b>The Haralick contrast of this image is 667.19 $bits$.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb6ab4-bc35-463b-8f1f-5b4f435bc54b",
   "metadata": {},
   "source": [
    "## How to compute Haralick features using the `mahotas` library\n",
    "\n",
    "The `mahotas` library computes all 14 Haralick features on any suitably processed image. This processing is usually done using `PIL` (the python image library) and `numpy`. The steps are:\n",
    "\n",
    "* Open the image using the `Image.open` command\n",
    "* Convert the image to `RGBA` format using `.convert(\"RGBA\")`\n",
    "* Convert to 8-$bit$ using `.convert(\"L\")`\n",
    "* Convert to grayscale using `ImageOps.grayscale`\n",
    "* Convery to `numpy` array using `np.asarray`\n",
    "\n",
    "Once this has been done, the Haralick features can be extracted using `mahotas`:\n",
    "\n",
    "* `features = mt.features.haralick(i, return_mean = True, compute_14th_feature=True)`\n",
    "\n",
    "In summary, the steps are:\n",
    "\n",
    "```\n",
    "img = Image.open(\"path_to_image.png\")\n",
    "img = img.convert(\"RGBA\"),\n",
    "img = img.convert(\"L\")\n",
    "img = ImageOps.grayscale(img)\n",
    "img = np.asanyarray(img)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7799d9a-51cb-4db9-8df9-ab2697a843bb",
   "metadata": {},
   "source": [
    "## Exercise: Get the Haralick entropy and contrast of each of these images\n",
    "\n",
    "![Sam](beckett.jpeg)\n",
    "\n",
    "![Jim](joyce.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea4d23-5c54-402b-82e1-aa50b2aa4353",
   "metadata": {},
   "source": [
    "# Can we predict image textures from features that are not themselves visual?\n",
    "\n",
    "Here, we are going to make a risky claim and test it. Specifically, we are going to test whether we we can predict the visual features of national flags from the demographic and geographical characteristics of the countries they represent. This will come in the form of three hypotheses:\n",
    "\n",
    "1. $H_1$: The greater the ethnic diversity of a country, the higher the entropy of its flag\n",
    "2. $H_2$: The more borders a country has, the lower the contrast of its flag\n",
    "3. $H_3$: Contrast will trump entropy as the optimised variable in most flags\n",
    "\n",
    "How can we test this? \n",
    "\n",
    "1. Get a [dataset of national flags](https://flagpedia.net/)\n",
    "2. Get the entropy and contrast of each flag\n",
    "3. Measure the ethnic diversity and number of borders of each country\n",
    "4. Statistically test whether ethnic diversity predicts entropy and number of borders predicts contrast\n",
    "5. Evaluate the distribution of flags in the entropy-contrast space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe54231",
   "metadata": {},
   "source": [
    "## Compute Haralick features of all our flags and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a5332-f1f5-4911-8a03-7a944de9c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('*.png')  # list of all .png files in the directory\n",
    "\n",
    "names = []\n",
    "images = []\n",
    "\n",
    "for i in filenames:\n",
    "    names.append(os.path.basename(i)[:-4])\n",
    "    img = Image.open(i)\n",
    "    img = img.convert(\"RGBA\")\n",
    "    img = img.convert(\"L\")\n",
    "    img = ImageOps.grayscale(img)\n",
    "    img = np.asanyarray(img)\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ab9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "haralick = [mt.features.haralick(i, return_mean = True, compute_14th_feature=True) for i in images]\n",
    "\n",
    "\n",
    "features = ['angular_2nd_momentum', 'contrast', 'correlation', 'SS_variance', \\\n",
    "            'Inverse_diff_moment', 'sum_average', 'sum_variance', 'sum_entropy', \\\n",
    "            'entropy','difference_variation', 'difference_entropy', 'info_corr_1', \\\n",
    "            'info_corr_2', 'max_corr_coeff']\n",
    "\n",
    "h_df = pd.DataFrame(haralick, columns = features)\n",
    "h_df['short_names'] = names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f6ad0",
   "metadata": {},
   "source": [
    "# Get full country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f067572",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name = []\n",
    "\n",
    "for i in h_df['short_names']:\n",
    "    try:\n",
    "        full_name.append(pycountry.countries.get(alpha_2=i).name)\n",
    "    except:\n",
    "        full_name.append(np.nan)\n",
    "        \n",
    "h_df['full_name'] = full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae14f3c-5562-47b0-a963-9f35ff9d0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633849e4",
   "metadata": {},
   "source": [
    "# Get number of borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "borders = []\n",
    "\n",
    "\n",
    "for i in h_df['full_name']:\n",
    "    try:\n",
    "        a = CountryInfo(i)\n",
    "        borders.append(len(a.borders()))\n",
    "    except:\n",
    "        borders.append(np.nan)\n",
    "\n",
    "h_df['borders'] = borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be0ceb-36a1-4533-9bb3-7ac23c7ca46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b6e20",
   "metadata": {},
   "source": [
    "## Get data on ethnic diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baf236",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnic = pd.read_csv('ethnic_fractions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_df = pd.merge(h_df, ethnic, on='full_name', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9433d-419c-447c-becf-937acdae8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h_df[['ethnic fractionalization', 'borders', 'entropy', 'contrast', 'full_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c55347-829d-4527-96ba-d7bfd8d0bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(h_df, x = 'ethnic fractionalization', y = 'entropy', hover_data = ['full_name'], trendline = 'ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = pg.linear_regression(h_df['ethnic fractionalization'], h_df['entropy'], remove_na = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6837b5-2446-41ec-aa8f-5c4acd2d61ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a730443-f9e0-4a94-b4ce-7b6b1accb582",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(h_df, x = 'borders', y = 'contrast', hover_data = ['full_name', 'borders'], trendline = 'ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28968e93-9c46-4ebb-baff-28e77a4ff189",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = pg.linear_regression(h_df['borders'], h_df['contrast'], remove_na = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a8f07-4a6d-4d93-824d-f02cb807638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_e = (data['entropy'].min() + data['entropy'].max()) / 2\n",
    "mid_c = (data['contrast'].min() + data['contrast'].max()) / 2\n",
    "\n",
    "data['entropy'] = data['entropy'] - mid_e\n",
    "data['contrast'] = data['contrast'] - mid_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(data, x=\"entropy\", y=\"contrast\", hover_data = ['full_name', 'borders'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92396454-aae2-42c4-ac58-1a6dea5bacd8",
   "metadata": {},
   "source": [
    "## Write-up\n",
    "\n",
    "A more detailed exploration of these results [can be found in this preprint](https://www.preprints.org/frontend/manuscript/d4f0a405c00c26799d2872250c47fd12/download_pub)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
